{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time Profiling Plural.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZJhC1AwiEHgP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "uetS0P2CEtYP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Kaiming He Initialization for kernel nitialization\n",
        "def init_kaiming_he(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "metadata": {
        "id": "OHB2dIEFE7vU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBN(tf.keras.Model):\n",
        "  def __init__(self, c_out):\n",
        "    super().__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=init_kaiming_he, use_bias=False)\n",
        "    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.nn.relu(self.bn(self.conv(inputs)))"
      ],
      "metadata": {
        "id": "uiwQnR6PE9rO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlk(tf.keras.Model):\n",
        "  def __init__(self, c_out, pool, res = False):\n",
        "    super().__init__()\n",
        "    self.conv_bn = ConvBN(c_out)\n",
        "    self.pool = pool\n",
        "    self.res = res\n",
        "    if self.res:\n",
        "      self.res1 = ConvBN(c_out)\n",
        "      self.res2 = ConvBN(c_out)\n",
        "      \n",
        "  def call(self, inputs):\n",
        "    h = self.pool(self.conv_bn(inputs))\n",
        "    if self.res:\n",
        "      h = h + self.res2(self.res1(h))\n",
        "    return h Creating model using subclassing API. \n",
        "# Same can be achieved using Functional API or Sequential API\n",
        "class ConvBN(tf.keras.Model):\n",
        "  def __init__(self, c_out):\n",
        "    super().__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=init_kaiming_he, use_bias=False)\n",
        "    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.nn.relu(self.bn(self.conv(inputs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "MVXpXh2CFA8X",
        "outputId": "59fe0f3c-4ebb-4d5a-9d75-456776fc55ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-57ab43d59e2e>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    return h Creating model using subclassing API.\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10AutoGrad(tf.keras.Model):\n",
        "  def __init__(self, c=64, weight=0.125):\n",
        "    super().__init__()\n",
        "    self.maxpool = tf.keras.layers.MaxPooling2D()\n",
        "    self.init_conv_bn = ConvBN(c)\n",
        "    self.blk1 = ResBlk(c*3, self.maxpool, res = True)\n",
        "    self.blk2 = ResBlk(c*6, self.maxpool)\n",
        "    self.blk3 = ResBlk(c*9, self.maxpool, res = True)\n",
        "    self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
        "    self.linear = tf.keras.layers.Dense(10, kernel_initializer=init_kaiming_he, use_bias=False)\n",
        "    self.weight = weight\n",
        "# Building the model\n",
        "  def call(self, x, y):\n",
        "    h = self.pool(self.maxpool(self.blk3(self.blk2(self.blk1(self.init_conv_bn(x))))))\n",
        "    h = self.linear(h) * self.weight\n",
        "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y) \n",
        "    loss = tf.reduce_sum(ce)\n",
        "    correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float32))\n",
        "    return loss, correct"
      ],
      "metadata": {
        "id": "9HtfsMH9Fx-Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Uw80aNwHF7pW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = time.time()\n",
        "\n",
        "def fit():\n",
        "  # Create batches for test data\n",
        "  test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
        "  train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug,num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n",
        "  #summary_writer = tf.summary.create_file_writer('./log/{}'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
        "  train_summary_writer = tf.summary.create_file_writer('./logs/train')\n",
        "  test_summary_writer = tf.summary.create_file_writer('./logs/test')    \n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_loss = test_loss = train_acc = test_acc = 0.0\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.trace_on(graph=True, profiler=True) # To start capturing profiler in Tensorboard\n",
        "      tf.keras.backend.set_learning_phase(1)\n",
        "      for (x, y) in tqdm(train_set):# Iterate over the batches of train dataset objects .\n",
        "        with tf.GradientTape() as tape:\n",
        "          #logits,actual  value for this minibatch\n",
        "          loss, correct = model(x, y)\n",
        "          var = model.trainable_variables\n",
        "          grads = tape.gradient(loss, var)\n",
        "          opt.apply_gradients(zip(grads, var))\n",
        "          global_step.assign_add(1)\n",
        "        # Add extra losses created during this forward pass\n",
        "        train_loss += loss.numpy()\n",
        "        train_acc += correct.numpy()\n",
        "\n",
        "      tf.summary.scalar('train loss', train_loss/len_train, step=epoch)\n",
        "      tf.summary.scalar('train acc', (train_acc/len_train)*100, step=epoch)\n",
        "      tf.summary.trace_export(name=\"Train\", step=epoch,profiler_outdir='./logs/train/trace') # Close Profiling when we do export\n",
        "      tf.keras.backend.set_learning_phase(0)\n",
        "      \n",
        "    with test_summary_writer.as_default(): \n",
        "      tf.summary.trace_on(graph=True, profiler=True)\n",
        "      for (x, y) in test_set:#  Iterate over the batches of train dataset objects .\n",
        "\n",
        "        loss, correct = model(x, y)\n",
        "        test_loss += loss.numpy()\n",
        "        test_acc += correct.numpy()         \n",
        "      tf.summary.scalar('test loss', test_loss/len_test, step=epoch)\n",
        "      tf.summary.scalar('test acc', (test_acc/len_test)*100, step=epoch) \n",
        "      tf.summary.trace_export(name=\"Test\", step=epoch,profiler_outdir='./logs/test/trace')  \n",
        "\n",
        "      template = 'Epoch {}, lr:{:.3f},Train Loss: {:.3f},Train Accuracy: {:.3f}, Test Loss: {:.3f}, Test Accuracy: {:.3f},Time Taken: {:.2f}'\n",
        "      print (template.format(epoch+1,lr_schedule(epoch+1),train_loss/len_train,(train_acc/len_train)*100,test_loss/len_test,(test_acc/len_test)*100,time.time() - t))"
      ],
      "metadata": {
        "id": "jFnANwYkF5mA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.summary.create_file_writer('./logs/train') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN33M9AZGEgL",
        "outputId": "bb07efdb-bb94-4428-b814-e912c958b7da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.ops.summary_ops_v2._ResourceSummaryWriter at 0x7f97071a2d10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "      print (template.format(epoch+1,lr_schedule(epoch+1),train_loss/len_train,(train_acc/len_train)*100,test_loss/len_test,(test_acc/len_test)*100,time.time() - t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "G9RwQH7hGxpo",
        "outputId": "fc76910e-57ac-4405-c103-363cec2bd13e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-14ac6d789d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'template' is not defined"
          ]
        }
      ]
    }
  ]
}