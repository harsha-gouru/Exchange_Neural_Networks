{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Time Profiling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms as T\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import argparse\n"
      ],
      "metadata": {
        "id": "Qx1LqIfsphi1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "# or any of these variants\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "from torchvision import datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wziZBD7mpkc6",
        "outputId": "99146a62-0487-491a-c85d-153323a7ad53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2eLFLBepqVN",
        "outputId": "fd786ac5-6cb0-49f8-964c-099aa2bb2bb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dataset\n",
            "  Downloading dataset-1.5.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting banal>=1.0.1\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from dataset) (1.4.35)\n",
            "Collecting alembic>=0.6.2\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->dataset) (4.11.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->dataset) (5.7.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.2->dataset) (1.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->dataset) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.0.1)\n",
            "Installing collected packages: Mako, banal, alembic, dataset\n",
            "Successfully installed Mako-1.2.0 alembic-1.7.7 banal-1.0.6 dataset-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start the time\n",
        "t = time.time()\n"
      ],
      "metadata": {
        "id": "7iD8kiJYqYEP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit():\n",
        "    # Create batches for test data\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug,num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n",
        "    #summary_writer = tf.summary.create_file_writer('./log/{}'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
        "    train_summary_writer = tf.summary.create_file_writer('./logs/train')\n",
        "    test_summary_writer = tf.summary.create_file_writer('./logs/test')    \n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = test_loss = train_acc = test_acc = 0.0\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.trace_on(graph=True, profiler=True) # To start capturing profiler in Tensorboard\n",
        "            tf.keras.backend.set_learning_phase(1)\n",
        "            for (x, y) in tqdm(train_set):# Iterate over the batches of train dataset objects .\n",
        "                with tf.GradientTape() as tape:\n",
        "                    #logits,actual  value for this minibatch\n",
        "                    loss, correct = model(x, y)\n",
        "                    var = model.trainable_variables\n",
        "                    grads = tape.gradient(loss, var)\n",
        "                    opt.apply_gradients(zip(grads, var))\n",
        "                    global_step.assign_add(1)\n",
        "                # Add extra losses created during this forward pass\n",
        "                train_loss += loss.numpy()\n",
        "                train_acc += correct.numpy()\n",
        "\n",
        "            tf.summary.scalar('train loss', train_loss/len_train, step=epoch)\n",
        "            tf.summary.scalar('train acc', train_acc/len_train, step=epoch)\n",
        "            tf.summary.trace_export(name=\"train_trace\", step=epoch)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.trace_on(graph=True, profiler=True)\n",
        "            tf.keras.backend.set_learning_phase(0)\n",
        "            for (x, y) in tqdm(test_set):\n",
        "                loss, correct = model(x, y)\n",
        "                test_loss += loss.numpy()\n",
        "                test_acc += correct.numpy()\n",
        "            tf.summary.scalar('test loss', test_loss/len_test, step=epoch)\n",
        "            tf.summary.scalar('test acc', test_acc/len_test, step=epoch)\n",
        "            tf.summary.trace_export(name=\"test_trace\", step=epoch)\n",
        "            print('Epoch: {}/{}'.format(epoch+1, EPOCHS),\n",
        "              'Train Loss: {:.4f}'.format(train_loss/len_train),\n",
        "              'Train Acc: {:.4f}'.format(train_acc/len_train),\n",
        "              'Test Loss: {:.4f}'.format(test_loss/len_test),\n",
        "              'Test Acc: {:.4f}'.format(test_acc/len_test))\n",
        "        # Save the model every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model.save_weights('./checkpoints/ckpt_{}'.format(epoch+1))\n",
        "                #end the time\n",
        "    print('Time taken for training: {} seconds'.format(time.time()-t))"
      ],
      "metadata": {
        "id": "ppX92S8KqdJh"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}